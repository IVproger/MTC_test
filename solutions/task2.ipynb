{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Материалы для работы:\n",
    "* https://habr.com/ru/articles/768562/\n",
    "* https://huggingface.co/learn/audio-course/ru/chapter5/asr_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Преимущества решения\n",
    "\n",
    "1. Простая реализация.\n",
    "2. За счет large модели, можно работать с разными языками.\n",
    "3. Качественное распознавание речи в аудио.\n",
    "4. Большой потенциал для дообучения модели (в завимисмотси от контекста решаемой нами задачи) -> ускоренное обучение и экономия ресурсов.\n",
    "\n",
    "# Недостатки решения\n",
    "1. Тесты проводились на хорошо предобработанных и чистых данных, что очень редко бывает в условиях продакшина. Необходимо в зависимости от контекста задачи добавить этап с предобработкой данных (убрать шумы, выделить голоса, изменить формат кодировки и тд). Не смотря на то, что сама whisper модель обучена на большом массиве разнообразных аудио, необходимо держать во внимании указанный недостаток. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решение задачи для аудио на Английском"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# Скачиваем модель для распознавания аудио\n",
    "# Medium model работает только с английским языком, если необходимо использовать другой язык, то нужно скачать large модель\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\", model=\"openai/whisper-medium\", device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "from torchaudio.datasets import LIBRISPEECH\n",
    "import shutil\n",
    "import warnings\n",
    "import difflib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 331M/331M [00:44<00:00, 7.80MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test audio WAV file and corresponding text file created successfully.\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"./data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "#  Скачиваем 1 инстанс датасета для теста нашей модели\n",
    "try:\n",
    "    dataset = LIBRISPEECH(data_dir, url=\"test-clean\", download=True)\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "sample = dataset[0]\n",
    "waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id = sample\n",
    "\n",
    "# Определяем директорию для сохранения аудиофайла\n",
    "output_dir = \"../audio\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Сохраняем аудиофайл в формате WAV\n",
    "torchaudio.save(os.path.join(output_dir, \"test_audio.wav\"), waveform, sample_rate)\n",
    "\n",
    "# Сохраняем текстовый файл с транскрипцией аудиофайла\n",
    "with open(os.path.join(output_dir, \"test_audio.txt\"), \"w\") as f:\n",
    "    f.write(utterance)\n",
    "\n",
    "# Удаляем директорию с данными для сохранения памяти\n",
    "shutil.rmtree(data_dir)\n",
    "print(\"Загрузка данных завершена\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Транскрипция:  he hoped there would be stew for dinner, turnips and carrots and bruised potatoes and fat mutton-pieces to be ladled out in thick, peppered, flour-fattened sauce.\n",
      "Оригинальный текст: he hoped there would be stew for dinner turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out in thick peppered flour fattened sauce\n",
      "Коэффициет сходства: 97.196 %\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Делаем транскрипцию аудиофайла\n",
    "result = pipe('../audio/test_audio.wav', max_new_tokens=256)\n",
    "\n",
    "# Выполняем сравнение транскрипции с оригинальным текстом\n",
    "transcribed_text = result[\"text\"].lower()\n",
    "print(f\"Транскрипция: {transcribed_text}\")\n",
    "\n",
    "with open('../audio/test_audio.txt') as file:\n",
    "    original_text = file.read().lower()\n",
    "print(f\"Оригинальный текст: {original_text}\")\n",
    "\n",
    "# Подсчитываем коэффициент сходства\n",
    "similarity_ratio = difflib.SequenceMatcher(None, transcribed_text, original_text).ratio()\n",
    "\n",
    "print(\"Коэффициет сходства:\", round(similarity_ratio*100,3), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решение задачи для аудио на Русском языке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae443fb1a8c491489f8d088bd84ee72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.99k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2297ae5a35da47f8a2c8aed847d28193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "201e9903fe4b462996f67cb080fe1cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/3.85k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a518100bc046d2a835cf18a66f884f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de696b1ceef44bc592cb6863b27a4729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86307742537643bfa2f9a9a07aeb4a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f4bd28bdee44e3b4dbd64769d851da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6efee93a375b4ae8b8d2c8c9b9d1aad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f10f4a6990964e71897dd66e33b8124b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a5ea9d256e04043bbfc8d06c84b9af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e3aa390a2143ecb22bc199bcf716e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# Скачиваем модель для распознавания аудио\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\", model=\"openai/whisper-large\", device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Транскрипция:  привет, мир! это тестовое аудио для проверки корректности распознавания речи большой языковой моделью.\n",
      "Оригинальный текст: привет мир это тестовое аудио для проверки корректности распознавания речи большой языковой моделью\n",
      "Коэффициет сходства: 98.02 %\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Делаем транскрипцию аудиофайла\n",
    "result = pipe('../audio/test_audion_rus.wav', max_new_tokens=256)\n",
    "\n",
    "# Выполняем сравнение транскрипции с оригинальным текстом\n",
    "transcribed_text = result[\"text\"].lower()\n",
    "print(f\"Транскрипция: {transcribed_text}\")\n",
    "\n",
    "with open('../audio/test_audio_rus.txt') as file:\n",
    "    original_text = file.read().lower()\n",
    "print(f\"Оригинальный текст: {original_text}\")\n",
    "\n",
    "# Подсчитываем коэффициент сходства\n",
    "similarity_ratio = difflib.SequenceMatcher(None, transcribed_text, original_text).ratio()\n",
    "\n",
    "print(\"Коэффициет сходства:\", round(similarity_ratio*100,3), \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
